{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "%pip install mlxtend\n",
    "\n",
    "# %%\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from mlxtend.frequent_patterns import association_rules, apriori # for mining frequent itemsets and association rule\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# %%\n",
    "file_path = 'Basket Analysis NoDups.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.columns\n",
    "\n",
    "# %%\n",
    "# counting the number of unique claims\n",
    "print('The total number of unique claims is ', df['Family ID'].nunique())\n",
    "\n",
    "# %%\n",
    "print(df.dtypes)\n",
    "\n",
    "# %%\n",
    "# convert date_time column into the right format for easier extracting\n",
    "df['date_column'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "# %%\n",
    "# extracting month and display full name of the month\n",
    "df['month'] = df['date_column'].dt.strftime('%Y-%m')\n",
    "\n",
    "# %%\n",
    "# List of specific columns you want to keep\n",
    "specific_columns = ['Family ID', 'LA', 'month', 'Crit-1',\n",
    "       'Crit-2', 'Crit-3', 'Crit-4', 'Crit-5', 'Crit-6', 'Crit-7', 'Crit-8',\n",
    "       'Crit-9', 'Crit-10']\n",
    "\n",
    "# Create a DataFrame with specific columns\n",
    "df_specific = pd.DataFrame({col: df[col] for col in specific_columns})\n",
    "'''\n",
    "# List of specific columns you want to keep\n",
    "specific_columns2 = ['Family ID', 'LA', 'month', 'C 1.1',\n",
    "       'C 1.2', 'C 1.3', 'C 1.4', 'C 2.1', 'C 2.2', 'C 2.3', 'C 3.1', 'C 3.2',\n",
    "       'C 3.3', 'C 4.1', 'C 4.2', 'C 5.1', 'C 5.2', 'C 5.3', 'C 5.4', 'C 6.1',\n",
    "       'C 6.2', 'C 6.3', 'C 6.4', 'C 6.5', 'C 7.1', 'C 7.2', 'C 7.3', 'C 8.1',\n",
    "       'C 8.2', 'C 8.3', 'C 9.1', 'C 9.2', 'C 9.3', 'C 10.1', 'C 10.2',\n",
    "       'C 10.3']\n",
    "\n",
    "# Create a DataFrame with specific columns\n",
    "df_specific2 = pd.DataFrame({col: df[col] for col in specific_columns2})\n",
    "'''\n",
    "\n",
    "# %%\n",
    "# Unpivot the criteria columns\n",
    "df_unpivoted = pd.melt(df_specific, id_vars=['Family ID', 'LA', 'month'], var_name='Criteria', value_name='Criteria_Value')\n",
    "\n",
    "# Filter rows where Criteria_Value is 1\n",
    "df_unpivoted = df_unpivoted[df_unpivoted['Criteria_Value'] == 1]\n",
    "\n",
    "# Drop duplicate rows based on 'FamilyID' and 'Criteria'\n",
    "df_no_dup = df_unpivoted.drop_duplicates(subset=['Family ID', 'Criteria'])\n",
    "\n",
    "# Order by Family ID\n",
    "df_no_dup.sort_values(by='Family ID', ascending=False, inplace=True)\n",
    "\n",
    "# Reset index\n",
    "df_no_dup.reset_index(drop=True, inplace=True)\n",
    "\n",
    "'''''\n",
    "# Unpivot the criteria columns\n",
    "df_unpivoted2 = pd.melt(df_specific2, id_vars=['Family ID', 'LA', 'month'], var_name='C', value_name='Criteria_Value')\n",
    "\n",
    "# Filter rows where Criteria_Value is 1\n",
    "df_unpivoted2 = df_unpivoted2[df_unpivoted2['Criteria_Value'] == 1]\n",
    "\n",
    "# Reset index\n",
    "df_unpivoted2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# %%\n",
    "df_no_dup\n",
    "\n",
    "# %% [markdown]\n",
    "# Criteria Distribution\n",
    "\n",
    "# %%\n",
    "# most common occuring criteria\n",
    "top_items = pd.DataFrame(df_no_dup['Criteria'].value_counts(dropna=True, sort=True)).reset_index()\n",
    "top_items.columns = ['item', 'count']\n",
    "top_items['percentage'] = top_items['count'].apply(lambda x: x/top_items['count'].sum())\n",
    "top_items = top_items.head(10)\n",
    "top_items.head(10)\n",
    "\n",
    "# %%\n",
    "# create bar plot showing criteria distribution\n",
    "plt.figure(figsize=(12,5))\n",
    "sns.barplot(x = 'item', y = 'count', data = top_items, palette = 'viridis')\n",
    "plt.xlabel('Criteria', size = 12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Number of Claims', size = 12)\n",
    "plt.title('Criteria Distribution', size = 15)\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# Number of criteria per Claim\n",
    "\n",
    "# %%\n",
    "# create summary table of number of transactions by their number of items\n",
    "# count the number of items of each transaction\n",
    "items_num = df_no_dup.groupby('Family ID', as_index=False)['Criteria'].count()\n",
    "# pivot table to display the distribution of transactions by the number of items\n",
    "items_num = items_num.pivot_table(index='Criteria', values='Family ID', aggfunc='count')\n",
    "# calculate percentage\n",
    "items_num['percentage'] = items_num['Family ID'].apply(lambda x: x/items_num['Family ID'].sum())\n",
    "items_num\n",
    "\n",
    "# %% [markdown]\n",
    "# Apply Apriori Algorithm to implement Market Basket Analysis\n",
    "\n",
    "# %%\n",
    "df_no_dup\n",
    "\n",
    "# %%\n",
    "# select only required variables for modelling\n",
    "transactions = df_no_dup.groupby(['Family ID', 'Criteria'])['Criteria'].count().reset_index(name ='Count')\n",
    "transactions.head()\n",
    "\n",
    "# %%\n",
    "# first create a mxn matrice where m=transaction and n=items\n",
    "# each row represents whether the items was in a specific transaction or not (>=1 returns True (1), 0 returns 0)\n",
    "my_basket = transactions.pivot_table(index='Family ID', columns='Criteria', values='Count', aggfunc='any').fillna(0)\n",
    "\n",
    "my_basket.head()\n",
    "\n",
    "# %%\n",
    "# create frequent items df with itemsets and support columns by using `apriori` function\n",
    "frequent_items = apriori(my_basket, min_support = 0.01, use_colnames = True)\n",
    "frequent_items\n",
    "\n",
    "# %%\n",
    "# create the rules from frequent itemset generated above with min lift = 1.2\n",
    "rules = association_rules(frequent_items, metric = \"lift\", min_threshold = 1.2)\n",
    "rules.sort_values('confidence', ascending = False, inplace = True)\n",
    "rules.reset_index(drop=True, inplace = True)\n",
    "rules\n",
    "\n",
    "# %% [markdown]\n",
    "# Parallel coordinates plot\n",
    "\n",
    "# %%\n",
    "# Function to convert rules to coordinates.\n",
    "def rules_to_coordinates(rules):\n",
    "    rules['antecedent'] = rules['antecedents'].apply(lambda antecedent: list(antecedent)[0])\n",
    "    rules['consequent'] = rules['consequents'].apply(lambda consequent: list(consequent)[0])\n",
    "    rules['rule'] = rules.index\n",
    "    return rules[['antecedent','consequent','rule']]\n",
    "\n",
    "# import sub lib to plot parallel coordinates\n",
    "from pandas.plotting import parallel_coordinates\n",
    "\n",
    "# Convert rules into coordinates suitable for use in a parallel coordinates plot\n",
    "coords = rules_to_coordinates(rules)\n",
    "\n",
    "# Generate parallel coordinates plot\n",
    "plt.figure(figsize=(5,5))\n",
    "parallel_coordinates(coords, 'rule')\n",
    "plt.legend([])\n",
    "plt.grid(ls='--', lw='0.5')\n",
    "plt.title('Parallel coordinates', size = 15)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# convert antecedents and consequents into strings\n",
    "rules['antecedents'] = rules['antecedents'].apply(lambda a: ','.join(list(a)))\n",
    "rules['consequents'] = rules['consequents'].apply(lambda a: ','.join(list(a)))\n",
    "\n",
    "# transform antecedent, consequent, and support columns into matrix\n",
    "support_table_lift = rules.pivot(index='consequents', columns='antecedents', values='lift')\n",
    "\n",
    "# generate a heatmap with annotations \n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(support_table_lift, annot = True, cbar = True, cmap=\"RdPu\")\n",
    "plt.suptitle('Itemsets Lift', size = 15)\n",
    "plt.title('How many times the antecedents and the consequents occur together more often than random?\\n', size=10)\n",
    "plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# Itemsets Confidence Heatmap\n",
    "\n",
    "# %%\n",
    "# transform antecedent, consequent, and support columns into matrix\n",
    "rules_confidence = rules[rules['confidence']>=0.2] # select min lift=1.2\n",
    "support_table_conf = rules_confidence.pivot(index='consequents', columns='antecedents', values='confidence')\n",
    "\n",
    "# generate a heatmap with annotations\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(support_table_conf, annot = True, cbar = True, cmap=\"BuPu\")\n",
    "plt.suptitle('Itemsets Confidence', size = 15)\n",
    "plt.title('How often the consequents is purchased given that the antecedents was purchased?\\n', size = 10)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
